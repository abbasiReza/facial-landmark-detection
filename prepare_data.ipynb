{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "json_files=os.listdir(\"C:/Users/user01/Documents/landmark/landmark detection data/all jsons\")\n",
    "json_files=json_files[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Open the file in write mode\n",
    "for file in json_files:\n",
    "    with open(\"C:/Users/user01/Documents/landmark/landmark detection data/all jsons/\"+file, 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "        data = json.load(f)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[]\n",
    "for a in data['shapes']:\n",
    "    tags.append(a['label'])\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files=os.listdir('C:/Users/user01/Documents/landmark/landmark detection data/all jsons/test set')\n",
    "len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Open the file in write mode\n",
    "for file in test_files:\n",
    "    with open('C:/Users/user01/Documents/landmark/landmark detection data/all jsons/test set/'+file, 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "        data = json.load(f)\n",
    "    print(file[:file.index('.')])\n",
    "    with open('our_dataset/test/'+file[:file.index('.')]+'.pts', 'w') as f:\n",
    "\n",
    "        f.write(\"version: 1\\n\")\n",
    "        f.write('n_points:  37\\n')\n",
    "        f.write('{\\n')\n",
    "        for tag in tags:\n",
    "            for a in data['shapes']:\n",
    "                if a['label']==tag:\n",
    "                    \n",
    "                    f.write('{} {}\\n'.format(a['points'][0][0],a['points'][0][1]))\n",
    "        f.write('}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from torchlm.data import LandmarksWFLWConverter\n",
    "from torchlm.data import Landmarks300WConverter\n",
    "from torchlm.data import LandmarksCOFWConverter\n",
    "from torchlm.data import LandmarksAFLWConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_300w_converter():\n",
    "    converter = Landmarks300WConverter(\n",
    "        data_dir=\"E:/code/landmark/our_dataset\",\n",
    "        save_dir=\"E:/code/landmark/our_dataset\",\n",
    "        extend=0.2,\n",
    "        rebuild=True,\n",
    "        target_size=256,\n",
    "        keep_aspect=False,\n",
    "        force_normalize=True,\n",
    "        force_absolute_path=True\n",
    "    )  \n",
    "    converter.convert()\n",
    "\n",
    "    converter.show(count=30)\n",
    "test_300w_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchlm.models import pipnet\n",
    "import torchlm\n",
    "import torch\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from torchlm.models import pipnet\n",
    "# will auto download pretrained weights from latest release if pretrained=True\n",
    "model = pipnet(backbone=\"resnet101\", pretrained=True, num_nb=10, num_lms=68, net_stride=32,\n",
    "               input_size=256, meanface_type=\"300w\", backbone_pretrained=True)\n",
    "\n",
    "\n",
    "#  \n",
    "model.apply_freezing(backbone=False)\n",
    "# generate your custom meanface.\n",
    "custom_meanface, custom_meanface_string = torchlm.data.annotools.generate_meanface(\n",
    "  annotation_path=\"E:/code/landmark/our_dataset/train.txt\",\n",
    "  coordinates_already_normalized=True)\n",
    "\n",
    "# setting up your custom meanface\n",
    "model.set_custom_meanface(custom_meanface_file_or_string=custom_meanface_string)\n",
    "model.apply_training(\n",
    "    annotation_path=\"E:/code/landmark/our_dataset/train.txt\",  # or fine-tuning your custom data\n",
    "    num_epochs=150,\n",
    "    learning_rate=3e-4,\n",
    "    save_dir=\"./save/pipnet\",\n",
    "    save_prefix=\"pipnet-wflw-resnet101\",\n",
    "    save_interval=2,\n",
    "    logging_interval=1,\n",
    "    device=\"cuda\",\n",
    "    coordinates_already_normalized=True,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# model.load_state_dict(torch.load('model_state150.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/code/landmark/our_dataset/train.txt') as f:\n",
    "    lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('E:/code/landmark/our_dataset/train.txt',delimiter=' ',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "all_sum=[]\n",
    "for i in range(len(df)):\n",
    "    img = plt.imread(df.iloc[i][0])\n",
    "    x_g=[]\n",
    "    y_g=[]\n",
    "    for j in range(1,len(df.iloc[i]),2):\n",
    "        x_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    for j in range(2,len(df.iloc[i]),2):\n",
    "        y_g.append(float(df.iloc[i][j])*256)\n",
    "\n",
    "    model.apply_detecting(image=img)\n",
    "    xx=torch.load('lms_pred_x.pt')*256\n",
    "    yy=torch.load('lms_pred_y.pt')*256\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for a in xx.cpu():\n",
    "        x.append(a.item())\n",
    "    for a in yy:\n",
    "        y.append(a.item())\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image using imshow()\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Generate some random x and y coordinates\n",
    "    # x = np.random.randint(0, img.shape[1], size=50)\n",
    "    # y = np.random.randint(0, img.shape[0], size=50)\n",
    "\n",
    "    # Plot the points using scatter()\n",
    "    k=20\n",
    "    # ax.scatter(x[k], y[k], c='r')\n",
    "    # ax.scatter(x_g[15], y_g[15], c='g')\n",
    "    ax.scatter(x, y, c='r')\n",
    "    # ax.scatter(x_g, y_g, c='g')\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    import math\n",
    "    import numpy as np\n",
    "    y_idexed=[]\n",
    "    sums=[]\n",
    "    for i in range(len(x)):\n",
    "        min=+9999\n",
    "        index=-1\n",
    "        for j in range(len(x_g)):\n",
    "            # if j in y_idexed:\n",
    "            #     continue\n",
    "            p1=[x[i],y[i]]\n",
    "            p2=[x_g[j],y_g[j]]\n",
    "            if abs(math.dist(p1,p2))<min:\n",
    "                min=abs(math.dist(p1,p2))\n",
    "                index=j\n",
    "        # if index not in y_idexed:\n",
    "        y_idexed.append(index)\n",
    "        sums.append(math.dist([x[i],y[i]],[x_g[index],y_g[index]]))\n",
    "        # print(i,index)\n",
    "    # print(np.mean(sums))\n",
    "    print(len(y_idexed))\n",
    "    all_sum.append(np.mean(sums))\n",
    "print(np.mean(all_sum))\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(),'model_state_150_resnet101v2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['file_name', 'width', 'height', \"Tr'_x\", \"Tr'_y\", \"Ft'R_x\", \"Ft'R_y\",\n",
    "       \"Ft'L_x\", \"Ft'L_y\", \"MBR_x\", \"MBR_y\", 'MBL_x', 'MBL_y', 'EAR_x',\n",
    "       'EAR_y', 'EAL_x', 'EAL_y', 'ExR_x', 'ExR_y', 'ExL_x', 'ExL_y', 'EnR_x',\n",
    "       'EnR_y', 'EnL_x', 'EnL_y', 'PupL_x', 'PupL_y', 'PupR_x', 'PupR_y',\n",
    "       'MfR_x', 'MfR_y', 'MfL_x', 'MfL_y', 'MER_x', 'MER_y', 'MEL_x', 'MEL_y',\n",
    "       \"Zy'R_x\", \"Zy'R_y\", \"Zy'L_x\", \"Zy'L_y\", 'Prn_x', 'Prn_y', 'AlR_x', 'AlR_y',\n",
    "       'AlL_x', 'AlL_y', 'Sn_x', 'Sn_y', 'Ls_x', 'Ls_y', 'CpR_x', 'CpR_y',\n",
    "       'CpL_x', 'CpL_y', 'Li_x', 'Li_y', 'Sbl_x', 'Sbl_y', 'ChR_x', 'ChR_y',\n",
    "       'ChL_x', 'ChL_y', \"Me'_x\", \"Me'_y\", \"Go'R_x\", \"Go'R_y\", \"Go'L_x\",\n",
    "       \"Go'L_y\", \"G'_x\", \"G'_y\", \"N'_x\", \"N'_y\", \"Sts_x\", 'Sts_y', 'Sti_x',\n",
    "       'Sti_y', 'St_x', 'St_y']\n",
    "\n",
    "pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['shapes'][10]['label'] in cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in data['shapes']:\n",
    "    print(cols.index(a['label']+'_x'))\n",
    "    print(cols.index(a['label']+'_y'))\n",
    "\n",
    "    # if a['label']+'_x' not in cols:\n",
    "        # print(a['label']+'_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( \"0.09344842654783869 0.15406877348086334 0.9323471108969382 0.1401049083310136 0.3997603796160685 0.15490567536186625 0.6237975504925628 0.1506668314387944 0.23501464872024747 0.10114569063499716 0.23657257629382072 0.14273999515886135 0.7844072096492979 0.09255827995641538 0.7886209940367209 0.13342958152383141 0.1702434206343547 0.2398254983581864 0.8642419047335136 0.23100885422325879 0.3779708093247684 0.24535590755251818 0.6567150531285483 0.24095057462080818 0.27793516230173154 0.20089460792973152 0.27633082606975745 0.26563845764538907 0.7570995084865105 0.19361548230508704 0.7635889825823677 0.2583984281705357 0.279266369046543 0.2302294117140321 0.7575495684331521 0.22349084426991003 0.38280645505717537 0.4925663990251028 0.6614317708106371 0.488658165265478 0.5189481268032609 0.4708399901570863 0.5225011032592264 0.539587476025657 0.32470035972074895 0.6648848905553355 0.7295419104987368 0.6593261745721221 0.5229449400615745 0.6265737161799486 0.5255912932309097 0.662324568779506 0.527155537232314 0.6983437257351308 0.5293851188286474 0.7515984253648098 0.5353511486450372 0.9458980717418055\".split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  -qO- bench.sh| bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BRACS2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
